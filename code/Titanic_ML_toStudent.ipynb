{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ **Titanic Machine Learning Models Comparison**  \n",
        "\n",
        "## By Kao Panboonyuen\n",
        "\n",
        "### This Colab notebook will guide you through:\n",
        "\n",
        "* ‚úÖ Loading the Titanic dataset\n",
        "* ‚úÖ Exploring and analyzing the data (EDA)\n",
        "* ‚úÖ Splitting the data into training and testing sets\n",
        "* ‚úÖ Training 3 machine learning models:\n",
        "     - üèóÔ∏è **Random Forest Classifier**\n",
        "     - üî• **Gradient Boosting Classifier**\n",
        "     - üèÜ **Neural Network Classifier**\n",
        "* ‚úÖ Evaluating model performance using:\n",
        "     - üìä **Confusion Matrix**\n",
        "     - üìâ **Precision, Recall, F1-Score**\n",
        "* ‚úÖ Comparing the performance of each model to find the best one"
      ],
      "metadata": {
        "id": "MhAbADlGPhZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 1: Import Required Libraries**  \n",
        "\n",
        "To get started, we'll need several libraries for data manipulation, visualization, and machine learning tasks."
      ],
      "metadata": {
        "id": "pE9zn_u6PkNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vr4F1S4PZGn"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 2: Load the Titanic Dataset**  \n",
        "\n",
        "Now, let's load the Titanic dataset from the provided GitHub URL."
      ],
      "metadata": {
        "id": "6tIOeK5wPnh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic dataset from the provided link\n",
        "url = 'https://github.com/kaopanboonyuen/OCSB-AI/blob/main/dataset/titanic-dataset.csv?raw=true'\n",
        "# Write your code here\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "xYEL46YCPmI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 3: Exploratory Data Analysis (EDA) üîç**  \n",
        "\n",
        "Before diving into the models, it's important to understand our data. We will look for missing values, visualize some basic distributions, and explore the relationships between different features."
      ],
      "metadata": {
        "id": "vJw0HxhbPryf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Checking for Missing Values\n",
        "\n",
        "We should start by checking for any missing values in the dataset."
      ],
      "metadata": {
        "id": "TJxJnybmPuEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "0Pu5_SHWPqDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Visualizing Data Distributions üìä\n",
        "\n",
        "Let's visualize the distribution of the \"Survived\" column (target variable) and some other features."
      ],
      "metadata": {
        "id": "1X9IrtnAPyOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the target variable \"Survived\"\n",
        "sns.countplot(x='Survived', data=data)\n",
        "plt.title('Distribution of Survived')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the distribution of age\n",
        "sns.histplot(data['Age'].dropna(), kde=True)\n",
        "plt.title('Age Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0uW-zDqtPwPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 4: Preprocess the Data ‚öôÔ∏è**  \n",
        "\n",
        "We need to handle missing values and convert categorical features into numerical ones. We will also split the dataset into features and labels."
      ],
      "metadata": {
        "id": "E3q0twJFP1Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values (impute or drop)\n",
        "data['Age'].fillna(data['Age'].mean(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical features (Sex and Embarked) to numeric\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Features and target variable\n",
        "X = data.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1)\n",
        "y = data['Survived']"
      ],
      "metadata": {
        "id": "2hVplU13Pzp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 5: Split the Data into Training and Testing Sets (with Fixed Seed)**  \n",
        "\n",
        "Now, let's split the data into training and testing sets, ensuring we can reproduce the results by setting a random seed."
      ],
      "metadata": {
        "id": "v6Q4x6ZkP5Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data with a fixed seed for reproducibility\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "nWuBu6_TP3Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 6: Train and Evaluate the Models üèóÔ∏è**  \n",
        "\n",
        "We will now train three different machine learning models: Random Forest, Gradient Boosting, and Neural Network."
      ],
      "metadata": {
        "id": "pTYMkiUlP8WH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1 Random Forest Classifier"
      ],
      "metadata": {
        "id": "KBh3L3tSP-Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest Classifier\n",
        "# Write your code here\n",
        "\n",
        "# Evaluate the model\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "print(\"Random Forest Classifier - Classification Report:\\n\", classification_report(y_test, rf_pred, digits=4))"
      ],
      "metadata": {
        "id": "8EIdH0EOP686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2 Gradient Boosting Classifier üî•"
      ],
      "metadata": {
        "id": "tXd_FYaYQVMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Gradient Boosting Classifier\n",
        "# Write your code here\n",
        "\n",
        "# Evaluate the model\n",
        "gb_pred = gb_model.predict(X_test)\n",
        "print(\"Gradient Boosting Classifier - Classification Report:\\n\", classification_report(y_test, gb_pred, digits=4))"
      ],
      "metadata": {
        "id": "bpnNVYWSP_nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.3 Neural Network Classifier üèÜ"
      ],
      "metadata": {
        "id": "gYEE-63NQYeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Neural Network Classifier (MLP)\n",
        "# Write your code here\n",
        "\n",
        "# Evaluate the model\n",
        "nn_pred = nn_model.predict(X_test)\n",
        "print(\"Neural Network Classifier - Classification Report:\\n\", classification_report(y_test, nn_pred, digits=4))"
      ],
      "metadata": {
        "id": "WR-XypWzQXTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 7: Confusion Matrix and Performance Comparison üìä**  \n",
        "\n",
        "Let's create confusion matrices and compare the F1-score, Precision, and Recall for each model."
      ],
      "metadata": {
        "id": "FTyrYZ7RQcUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.1 Confusion Matrix"
      ],
      "metadata": {
        "id": "MX3R3vTlQdpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Random Forest\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, rf_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for Gradient Boosting\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, gb_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Gradient Boosting Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for Neural Network\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, nn_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Neural Network Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vu3scfWWQaA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.2 Performance Metrics Comparison"
      ],
      "metadata": {
        "id": "Rqv4Jmt9QgW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_rf = f1_score(y_test, rf_pred)\n",
        "f1_gb = f1_score(y_test, gb_pred)\n",
        "f1_nn = f1_score(y_test, nn_pred)\n",
        "\n",
        "print(f\"Random Forest F1-Score: {f1_rf:.4f}\")\n",
        "print(f\"Gradient Boosting F1-Score: {f1_gb:.4f}\")\n",
        "print(f\"Neural Network F1-Score: {f1_nn:.4f}\")\n",
        "\n",
        "# Precision and Recall Comparison\n",
        "precision_rf = precision_score(y_test, rf_pred)\n",
        "precision_gb = precision_score(y_test, gb_pred)\n",
        "precision_nn = precision_score(y_test, nn_pred)\n",
        "\n",
        "recall_rf = recall_score(y_test, rf_pred)\n",
        "recall_gb = recall_score(y_test, gb_pred)\n",
        "recall_nn = recall_score(y_test, nn_pred)\n",
        "\n",
        "print(f\"Random Forest Precision: {precision_rf:.4f}, Recall: {recall_rf:.4f}\")\n",
        "print(f\"Gradient Boosting Precision: {precision_gb:.4f}, Recall: {recall_gb:.4f}\")\n",
        "print(f\"Neural Network Precision: {precision_nn:.4f}, Recall: {recall_nn:.4f}\")"
      ],
      "metadata": {
        "id": "2jsRmK-VQfHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Step 8: Model Comparison üèÜ**  "
      ],
      "metadata": {
        "id": "ND-vSlnLQj3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will compare the performance of the three models to determine which one performed the best.\n",
        "\n",
        "- Based on **F1-Score**, **Precision**, and **Recall**, we will decide the winning model.\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ **Conclusion**  \n",
        "\n",
        "In this notebook, we have:\n",
        "- Loaded and preprocessed the Titanic dataset.\n",
        "- Performed exploratory data analysis (EDA).\n",
        "- Built and evaluated three different machine learning models: Random Forest, Gradient Boosting, and Neural Networks.\n",
        "- Compared the performance of each model using confusion matrices and classification reports."
      ],
      "metadata": {
        "id": "RMPssU7pQm_X"
      }
    }
  ]
}